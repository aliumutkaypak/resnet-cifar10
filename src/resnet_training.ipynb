{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VIi6JeghfA0"
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6oL5rmU684KS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh1RE49dk46A"
   },
   "source": [
    "# Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "daazjLZok4ey"
   },
   "outputs": [],
   "source": [
    "# Dataset directories\n",
    "labeled_set_dir = 'data/cifar-10-batches-py'\n",
    "unlabeled_set_dir = 'data'\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Output csv file\n",
    "output_csv_path = 'out_5.csv'\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_path = 'model_5.pth'\n",
    "\n",
    "# Augmentation strategy\n",
    "augment = 'set2' #Choices: 'set2', None, 'set1'\n",
    "\n",
    "#learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "blocks_in_layers = [2, 1, 1, 1]\n",
    "num_layers = 4\n",
    "dr= 0.0 \n",
    "num_channels= [64, 128, 256, 512]\n",
    "avg_pool_kernel_s=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRxu_aQVhnoz"
   },
   "source": [
    "# Define CIFAR10 Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9sIFWbEE7MAS"
   },
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data_dir, train=True, unlabeled=False, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.unlabeled = unlabeled\n",
    "        # Load all data batches\n",
    "        if unlabeled:\n",
    "          self.data, self.id = self.load_unlabeled_data()\n",
    "          self.labels = None\n",
    "        else:\n",
    "          self.data, self.labels = self.load_labeled_data()\n",
    "\n",
    "    def load_cifar_batch(self, file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            batch = pickle.load(fo, encoding='bytes')\n",
    "        return batch\n",
    "\n",
    "    def load_labeled_data(self):\n",
    "        data_batches = []\n",
    "        label_batches = []\n",
    "        if self.train:\n",
    "          for i in range(1, 6):\n",
    "            batch_file = os.path.join(self.data_dir, f'data_batch_{i}')\n",
    "            batch = self.load_cifar_batch(batch_file)\n",
    "            data_batches.append(batch[b'data'])\n",
    "            label_batches += batch[b'labels']\n",
    "        else:\n",
    "          batch_file = os.path.join(self.data_dir, f'test_batch')\n",
    "          batch = self.load_cifar_batch(batch_file)\n",
    "          data_batches.append(batch[b'data'])\n",
    "          label_batches += batch[b'labels']\n",
    "\n",
    "        data = np.vstack(data_batches).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        labels = np.array(label_batches)\n",
    "        return data, labels\n",
    "\n",
    "    def load_unlabeled_data(self):\n",
    "        # Load the unlabeled batch\n",
    "        batch_file = os.path.join(self.data_dir, 'cifar_test_nolabels.pkl')\n",
    "        batch = self.load_cifar_batch(batch_file)\n",
    "        data = batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        id = batch[b'ids'].tolist()\n",
    "        return data, id\n",
    "\n",
    "    def __len__(self):\n",
    "        if not self.unlabeled:\n",
    "          return len(self.labels)\n",
    "        else:\n",
    "          return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img = self.data[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if not self.unlabeled:\n",
    "          label = self.labels[idx]\n",
    "          return img, label\n",
    "        else:\n",
    "          return img, self.id[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NUrv6SCh0UW"
   },
   "source": [
    "# Define Resnet class. Changed version of the Resnet class in this repo: https://github.com/kuangliu/pytorch-cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lNSnNUnKi_d6"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "  expansion = 1\n",
    "\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(BasicBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(\n",
    "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                            stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion*planes:\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*planes)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "  def __init__(self, block, num_blocks, num_layers = 4, dropout= 0.3 ,num_channels=[64, 128, 256, 512] , avg_pool_kernel_s=4, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    assert len(num_channels) == num_layers\n",
    "    assert len(num_blocks) == num_layers\n",
    "    self.in_planes = 64\n",
    "    self.avg_pool_kernel_s = avg_pool_kernel_s\n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                            stride=1, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "      stride = 1 if i == 0 else 2\n",
    "      layers.append(nn.Dropout2d(p=dropout))\n",
    "      layers.append( self._make_layer(block, num_channels[i], num_blocks[i], stride=stride))\n",
    "    self.layers = nn.Sequential(*layers)\n",
    "    self.linear = nn.Linear(num_channels[-1]*block.expansion, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "    strides = [stride] + [1]*(num_blocks-1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "        layers.append(block(self.in_planes, planes, stride))\n",
    "        self.in_planes = planes * block.expansion\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.layers(out)\n",
    "    out = F.avg_pool2d(out, self.avg_pool_kernel_s)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.linear(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ENMDE4tx12x",
    "outputId": "47812efb-6877-4102-c4f0-85b1e41d8ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# If GPU available, the code uses it. Otherwise cpu is used for the training (not recommended).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODmBAwVGw784"
   },
   "source": [
    "# Load dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RDmaYw8Rohnk"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "BATCH_SIZE_TRAIN = 128\n",
    "BATCH_SIZE_TEST = 1000\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10Dataset(labeled_set_dir, train=True, unlabeled=False, transform=transform)\n",
    "testset = CIFAR10Dataset(labeled_set_dir, train=False, unlabeled=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4un-sO_ijpm"
   },
   "source": [
    "## Create validation set from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5kVGBCe1vvPd"
   },
   "outputs": [],
   "source": [
    "total_size = len(trainset)\n",
    "train_size = int(0.9 * total_size)\n",
    "validation_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "trainset, validationset = torch.utils.data.random_split(trainset, [train_size, validation_size], generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpF-2GnaipuP"
   },
   "source": [
    "## Augment the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I3YCwu942W2J"
   },
   "outputs": [],
   "source": [
    "#Augmentation\n",
    "if augment is None:\n",
    "    train_transform = transform\n",
    "elif augment=='set1':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "elif augment=='set2':\n",
    "    train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(policy= transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "trainset.dataset.transform = train_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqWZRKC6itri"
   },
   "source": [
    "## Create dataloader instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9btjqfl5yq6W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE_TRAIN,shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=BATCH_SIZE_TEST,shuffle=False,  num_workers=1)\n",
    "validation_loader = torch.utils.data.DataLoader(validationset,batch_size=BATCH_SIZE_TRAIN,shuffle=False,  num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cuwl03CkizmT"
   },
   "source": [
    "# Define training and evaluating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NrbAmDZpua0n"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler, data_loader, loss_history, acc_history, criterion):\n",
    "  total_samples = len(data_loader.dataset)\n",
    "  model.train()\n",
    "  correct_samples = 0\n",
    "  losses = []\n",
    "  for i, (data, target) in enumerate(data_loader):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    correct_samples += pred.eq(target).sum()  \n",
    "\n",
    "  avg_loss = np.mean(losses)  \n",
    "  loss_history.append(avg_loss)\n",
    "  acc = correct_samples / total_samples\n",
    "  acc_history.append(acc.item())  \n",
    "  print('\\nAverage training loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "  ' Accuracy:' + '{:5}'.format(correct_samples) + '/' + '{:5}'.format(total_samples) + ' (' +\n",
    "  '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LtmwnJgk06_N"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_history, acc_history, criterion, set_name='test'):\n",
    "  model.eval()\n",
    "  total_samples = len(data_loader.dataset)\n",
    "  correct_samples = 0\n",
    "  losses = []\n",
    "  acc = []\n",
    "  with torch.no_grad():\n",
    "    for data, target in data_loader:\n",
    "      data = data.to(device)\n",
    "      target = target.to(device)\n",
    "      output = model(data)\n",
    "      loss = criterion(output, target)\n",
    "      _, pred = torch.max(output, dim=1)\n",
    "\n",
    "      losses.append(loss.item())\n",
    "      correct_samples += pred.eq(target).sum()\n",
    "\n",
    "  avg_loss = np.mean(losses)\n",
    "  loss_history.append(avg_loss)\n",
    "  acc = correct_samples / total_samples\n",
    "  acc_history.append(acc.item())\n",
    "  print('\\nAverage '+ set_name + ' loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "  ' Accuracy:' + '{:5}'.format(correct_samples) + '/' + '{:5}'.format(total_samples) + ' (' +\n",
    "  '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uia6PwY4jFEg"
   },
   "source": [
    "# Create model, optimizer, schedular, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ouhj4UI2RLS",
    "outputId": "18a9904b-3c36-4d3c-dea5-4cae64bab6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "         Dropout2d-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 32, 32]             128\n",
      "           Conv2d-11           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-13           [-1, 64, 32, 32]               0\n",
      "        Dropout2d-14           [-1, 64, 32, 32]               0\n",
      "           Conv2d-15          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
      "           Conv2d-17          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
      "           Conv2d-19          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-21          [-1, 128, 16, 16]               0\n",
      "        Dropout2d-22          [-1, 128, 16, 16]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "           Conv2d-25            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
      "           Conv2d-27            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-29            [-1, 256, 8, 8]               0\n",
      "        Dropout2d-30            [-1, 256, 8, 8]               0\n",
      "           Conv2d-31            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-33            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-35            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-36            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-37            [-1, 512, 4, 4]               0\n",
      "           Linear-38                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,977,226\n",
      "Trainable params: 4,977,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 10.44\n",
      "Params size (MB): 18.99\n",
      "Estimated Total Size (MB): 29.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(BasicBlock,\n",
    "               blocks_in_layers, \n",
    "               num_layers = num_layers, \n",
    "               dropout=dr,\n",
    "               num_channels=num_channels, \n",
    "               avg_pool_kernel_s=avg_pool_kernel_s, \n",
    "               num_classes=10)\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr,\n",
    "                     momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                 T_max=num_epochs)  # eta_min is the minimum lr\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCaMaVVCjIUn"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8EhRWr6pSIn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Average training loss: 2.2430 Accuracy: 7094/45000 (15.76%)\n",
      "\n",
      "\n",
      "Average validation loss: 2.1077 Accuracy: 1018/ 5000 (20.36%)\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Average training loss: 2.0824 Accuracy: 9988/45000 (22.20%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.9748 Accuracy: 1309/ 5000 (26.18%)\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Average training loss: 1.9831 Accuracy:11999/45000 (26.66%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.8922 Accuracy: 1546/ 5000 (30.92%)\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Average training loss: 1.8977 Accuracy:13451/45000 (29.89%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.7755 Accuracy: 1802/ 5000 (36.04%)\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Average training loss: 1.8144 Accuracy:14998/45000 (33.33%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.6990 Accuracy: 1966/ 5000 (39.32%)\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Average training loss: 1.7350 Accuracy:16523/45000 (36.72%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.5799 Accuracy: 2093/ 5000 (41.86%)\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Average training loss: 1.6270 Accuracy:18593/45000 (41.32%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.5398 Accuracy: 2194/ 5000 (43.88%)\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Average training loss: 1.5435 Accuracy:20246/45000 (44.99%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.3482 Accuracy: 2570/ 5000 (51.40%)\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Average training loss: 1.4720 Accuracy:21404/45000 (47.56%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.2985 Accuracy: 2689/ 5000 (53.78%)\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Average training loss: 1.4211 Accuracy:22307/45000 (49.57%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.2696 Accuracy: 2675/ 5000 (53.50%)\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Average training loss: 1.3646 Accuracy:23202/45000 (51.56%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.2084 Accuracy: 2823/ 5000 (56.46%)\n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Average training loss: 1.3243 Accuracy:23859/45000 (53.02%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.2048 Accuracy: 2771/ 5000 (55.42%)\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Average training loss: 1.2847 Accuracy:24466/45000 (54.37%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.1228 Accuracy: 3012/ 5000 (60.24%)\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Average training loss: 1.2453 Accuracy:25235/45000 (56.08%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.0760 Accuracy: 3099/ 5000 (61.98%)\n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Average training loss: 1.2131 Accuracy:25762/45000 (57.25%)\n",
      "\n",
      "\n",
      "Average validation loss: 1.0415 Accuracy: 3154/ 5000 (63.08%)\n",
      "\n",
      "Epoch: 16\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf  # Track the highest validation accuracy\n",
    "\n",
    "start_time = time.time()\n",
    "train_loss_history, valid_loss_history = [], []\n",
    "train_acc_history, valid_acc_history = [], []\n",
    "termination_count = 0 #If valid loss does not decrease in 5 consecutive epochs terminate the training\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  print('Epoch:', epoch)\n",
    "  train_epoch(model, optimizer, scheduler, train_loader, train_loss_history, train_acc_history, criterion)\n",
    "  evaluate(model, validation_loader, valid_loss_history, valid_acc_history, criterion, 'validation')\n",
    "  if valid_loss_history[-1] < best_val_loss:\n",
    "  #Save the best model in terms of validation loss.\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    best_val_loss = valid_loss_history[-1]\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlGkSTbXnJ5t"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gNh2mxzj1A2"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYXa9uWcmMTY"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history,'-',linewidth=3,label='Train error')\n",
    "plt.plot(valid_loss_history,'-',linewidth=3,label='Validation error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "%matplotlib inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_history,'-',linewidth=3,label='Train accuracy')\n",
    "plt.plot(valid_acc_history,'-',linewidth=3,label='Validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "%matplotlib inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDIQlkT7xovA",
    "outputId": "af51279b-9f21-4aa8-8ca8-6449a0c9a875"
   },
   "outputs": [],
   "source": [
    "evaluate(model, test_loader, [], criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEdBcwR4kGRd"
   },
   "source": [
    "# Predict labels for nonlabeled dataset and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPnMLwXeDR4r"
   },
   "outputs": [],
   "source": [
    "def predict_labels(model, data_loader):\n",
    "  model.eval()\n",
    "  predicted_labels = []\n",
    "  ids = []\n",
    "  with torch.no_grad():\n",
    "    for data, id in data_loader:\n",
    "      data = data.to(device)\n",
    "      output = model(data)\n",
    "      _, pred = torch.max(output, dim=1)\n",
    "      predicted_labels = predicted_labels + pred.tolist()\n",
    "      ids = ids + id.tolist()\n",
    "  df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Labels': predicted_labels\n",
    "})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-5KRy9SHqvK"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "unlabeled_testset = CIFAR10Dataset(unlabeled_set_dir, train=False, unlabeled=True, transform=transform)\n",
    "unlabeled_test_loader = torch.utils.data.DataLoader(unlabeled_testset,batch_size=BATCH_SIZE_TEST,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzQw8ijLJZHI"
   },
   "outputs": [],
   "source": [
    "df = predict_labels(model, unlabeled_test_loader)\n",
    "df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
