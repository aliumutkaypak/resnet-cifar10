{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import necessary libraries"
      ],
      "metadata": {
        "id": "-VIi6JeghfA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, Dataset\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "6oL5rmU684KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define training arguments"
      ],
      "metadata": {
        "id": "rh1RE49dk46A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset directories\n",
        "labeled_set_dir = '/content/cifar-10-batches-py'\n",
        "unlabeled_set_dir = '/content'\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 150\n",
        "\n",
        "# Output csv file\n",
        "output_csv_path = '/content/output.csv'\n",
        "\n",
        "# Checkpoint path\n",
        "checkpoint_path = 'best_model.pth'\n"
      ],
      "metadata": {
        "id": "daazjLZok4ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define CIFAR10 Dataset class"
      ],
      "metadata": {
        "id": "fRxu_aQVhnoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, data_dir, train=True, unlabeled=False, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        self.unlabeled = unlabeled\n",
        "        # Load all data batches\n",
        "        if unlabeled:\n",
        "          self.data, self.id = self.load_unlabeled_data()\n",
        "          self.labels = None\n",
        "        else:\n",
        "          self.data, self.labels = self.load_labeled_data()\n",
        "\n",
        "    def load_cifar_batch(self, file):\n",
        "        with open(file, 'rb') as fo:\n",
        "            batch = pickle.load(fo, encoding='bytes')\n",
        "        return batch\n",
        "\n",
        "    def load_labeled_data(self):\n",
        "        data_batches = []\n",
        "        label_batches = []\n",
        "        if self.train:\n",
        "          for i in range(1, 6):\n",
        "            batch_file = os.path.join(self.data_dir, f'data_batch_{i}')\n",
        "            batch = self.load_cifar_batch(batch_file)\n",
        "            data_batches.append(batch[b'data'])\n",
        "            label_batches += batch[b'labels']\n",
        "        else:\n",
        "          batch_file = os.path.join(self.data_dir, f'test_batch')\n",
        "          batch = self.load_cifar_batch(batch_file)\n",
        "          data_batches.append(batch[b'data'])\n",
        "          label_batches += batch[b'labels']\n",
        "\n",
        "        data = np.vstack(data_batches).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        labels = np.array(label_batches)\n",
        "        return data, labels\n",
        "\n",
        "    def load_unlabeled_data(self):\n",
        "        # Load the unlabeled batch\n",
        "        batch_file = os.path.join(self.data_dir, 'cifar_test_nolabels.pkl')\n",
        "        batch = self.load_cifar_batch(batch_file)\n",
        "        data = batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        id = batch[b'ids'].tolist()\n",
        "        return data, id\n",
        "\n",
        "    def __len__(self):\n",
        "        if not self.unlabeled:\n",
        "          return len(self.labels)\n",
        "        else:\n",
        "          return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img = self.data[idx]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if not self.unlabeled:\n",
        "          label = self.labels[idx]\n",
        "          return img, label\n",
        "        else:\n",
        "          return img, self.id[idx]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9sIFWbEE7MAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Resnet class. Changed version of the Resnet class in this repo: https://github.com/kuangliu/pytorch-cifar"
      ],
      "metadata": {
        "id": "6NUrv6SCh0UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                            stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                      kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(self.expansion*planes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_layers = 4, dropout= 0.3 ,num_channels=[64, 128, 256, 512] , avg_pool_kernel_s=4, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    assert len(num_channels) == num_layers\n",
        "    assert len(num_blocks) == num_layers\n",
        "    self.in_planes = 64\n",
        "    self.avg_pool_kernel_s = avg_pool_kernel_s\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                            stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    layers = []\n",
        "    for i in range(num_layers):\n",
        "      stride = 1 if i == 0 else 2\n",
        "      layers.append(nn.Dropout2d(p=dropout))\n",
        "      layers.append( self._make_layer(block, num_channels[i], num_blocks[i], stride=stride))\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "    self.linear = nn.Linear(num_channels[-1]*block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "        layers.append(block(self.in_planes, planes, stride))\n",
        "        self.in_planes = planes * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layers(out)\n",
        "    out = F.avg_pool2d(out, self.avg_pool_kernel_s)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "lNSnNUnKi_d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If GPU available, the code uses it. Otherwise cpu is used for the training (not recommended).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "0ENMDE4tx12x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47812efb-6877-4102-c4f0-85b1e41d8ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ODmBAwVGw784"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 1000\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = CIFAR10Dataset(labeled_set_dir, train=True, unlabeled=False, transform=transform)\n",
        "testset = CIFAR10Dataset(labeled_set_dir, train=False, unlabeled=False, transform=transform)"
      ],
      "metadata": {
        "id": "RDmaYw8Rohnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create validation set from the training set"
      ],
      "metadata": {
        "id": "_4un-sO_ijpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_size = len(trainset)\n",
        "train_size = int(0.9 * total_size)\n",
        "validation_size = total_size - train_size\n",
        "\n",
        "# Split the dataset\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "trainset, validationset = torch.utils.data.random_split(trainset, [train_size, validation_size], generator)\n"
      ],
      "metadata": {
        "id": "5kVGBCe1vvPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augment the training dataset"
      ],
      "metadata": {
        "id": "HpF-2GnaipuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "trainset.dataset.transform = train_transform"
      ],
      "metadata": {
        "id": "I3YCwu942W2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create dataloader instances"
      ],
      "metadata": {
        "id": "kqWZRKC6itri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE_TRAIN,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset,batch_size=BATCH_SIZE_TEST,shuffle=False)\n",
        "validation_loader = torch.utils.data.DataLoader(validationset,batch_size=BATCH_SIZE_TRAIN,shuffle=False)"
      ],
      "metadata": {
        "id": "9btjqfl5yq6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define training and evaluating functions"
      ],
      "metadata": {
        "id": "Cuwl03CkizmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, scheduler, data_loader, loss_history, criterion):\n",
        "  total_samples = len(data_loader.dataset)\n",
        "  model.train()\n",
        "  for i, (data, target) in enumerate(data_loader):\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0:\n",
        "      print('[' + '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "      ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)] Loss: ' +\n",
        "      '{:6.4f}'.format(loss.item()))\n",
        "  loss_history.append(loss.item())\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "NrbAmDZpua0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss_history, criterion):\n",
        "  model.eval()\n",
        "  total_samples = len(data_loader.dataset)\n",
        "  correct_samples = 0\n",
        "  losses = []\n",
        "  with torch.no_grad():\n",
        "    for data, target in data_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      _, pred = torch.max(output, dim=1)\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      correct_samples += pred.eq(target).sum()\n",
        "\n",
        "  avg_loss = np.mean(losses)\n",
        "  loss_history.append(avg_loss)\n",
        "  print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "  ' Accuracy:' + '{:5}'.format(correct_samples) + '/' + '{:5}'.format(total_samples) + ' (' +\n",
        "  '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "metadata": {
        "id": "LtmwnJgk06_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create model, optimizer, schedular, criterion"
      ],
      "metadata": {
        "id": "uia6PwY4jFEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(BasicBlock, [2, 1, 1, 1], num_layers = 4, dropout= 0.0 ,num_channels=[64, 128, 256, 512] , avg_pool_kernel_s=4, num_classes=10)\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ouhj4UI2RLS",
        "outputId": "18a9904b-3c36-4d3c-dea5-4cae64bab6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "         Dropout2d-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 42, 16, 16]          24,192\n",
            "       BatchNorm2d-5           [-1, 42, 16, 16]              84\n",
            "            Conv2d-6           [-1, 42, 16, 16]          15,876\n",
            "       BatchNorm2d-7           [-1, 42, 16, 16]              84\n",
            "            Conv2d-8           [-1, 42, 16, 16]           2,688\n",
            "       BatchNorm2d-9           [-1, 42, 16, 16]              84\n",
            "       BasicBlock-10           [-1, 42, 16, 16]               0\n",
            "           Conv2d-11           [-1, 42, 16, 16]          15,876\n",
            "      BatchNorm2d-12           [-1, 42, 16, 16]              84\n",
            "           Conv2d-13           [-1, 42, 16, 16]          15,876\n",
            "      BatchNorm2d-14           [-1, 42, 16, 16]              84\n",
            "       BasicBlock-15           [-1, 42, 16, 16]               0\n",
            "           Conv2d-16           [-1, 42, 16, 16]          15,876\n",
            "      BatchNorm2d-17           [-1, 42, 16, 16]              84\n",
            "           Conv2d-18           [-1, 42, 16, 16]          15,876\n",
            "      BatchNorm2d-19           [-1, 42, 16, 16]              84\n",
            "       BasicBlock-20           [-1, 42, 16, 16]               0\n",
            "        Dropout2d-21           [-1, 42, 16, 16]               0\n",
            "           Conv2d-22           [-1, 84, 16, 16]          31,752\n",
            "      BatchNorm2d-23           [-1, 84, 16, 16]             168\n",
            "           Conv2d-24           [-1, 84, 16, 16]          63,504\n",
            "      BatchNorm2d-25           [-1, 84, 16, 16]             168\n",
            "           Conv2d-26           [-1, 84, 16, 16]           3,528\n",
            "      BatchNorm2d-27           [-1, 84, 16, 16]             168\n",
            "       BasicBlock-28           [-1, 84, 16, 16]               0\n",
            "           Conv2d-29           [-1, 84, 16, 16]          63,504\n",
            "      BatchNorm2d-30           [-1, 84, 16, 16]             168\n",
            "           Conv2d-31           [-1, 84, 16, 16]          63,504\n",
            "      BatchNorm2d-32           [-1, 84, 16, 16]             168\n",
            "       BasicBlock-33           [-1, 84, 16, 16]               0\n",
            "           Conv2d-34           [-1, 84, 16, 16]          63,504\n",
            "      BatchNorm2d-35           [-1, 84, 16, 16]             168\n",
            "           Conv2d-36           [-1, 84, 16, 16]          63,504\n",
            "      BatchNorm2d-37           [-1, 84, 16, 16]             168\n",
            "       BasicBlock-38           [-1, 84, 16, 16]               0\n",
            "        Dropout2d-39           [-1, 84, 16, 16]               0\n",
            "           Conv2d-40            [-1, 168, 8, 8]         127,008\n",
            "      BatchNorm2d-41            [-1, 168, 8, 8]             336\n",
            "           Conv2d-42            [-1, 168, 8, 8]         254,016\n",
            "      BatchNorm2d-43            [-1, 168, 8, 8]             336\n",
            "           Conv2d-44            [-1, 168, 8, 8]          14,112\n",
            "      BatchNorm2d-45            [-1, 168, 8, 8]             336\n",
            "       BasicBlock-46            [-1, 168, 8, 8]               0\n",
            "           Conv2d-47            [-1, 168, 8, 8]         254,016\n",
            "      BatchNorm2d-48            [-1, 168, 8, 8]             336\n",
            "           Conv2d-49            [-1, 168, 8, 8]         254,016\n",
            "      BatchNorm2d-50            [-1, 168, 8, 8]             336\n",
            "       BasicBlock-51            [-1, 168, 8, 8]               0\n",
            "        Dropout2d-52            [-1, 168, 8, 8]               0\n",
            "           Conv2d-53            [-1, 336, 4, 4]         508,032\n",
            "      BatchNorm2d-54            [-1, 336, 4, 4]             672\n",
            "           Conv2d-55            [-1, 336, 4, 4]       1,016,064\n",
            "      BatchNorm2d-56            [-1, 336, 4, 4]             672\n",
            "           Conv2d-57            [-1, 336, 4, 4]          56,448\n",
            "      BatchNorm2d-58            [-1, 336, 4, 4]             672\n",
            "       BasicBlock-59            [-1, 336, 4, 4]               0\n",
            "           Conv2d-60            [-1, 336, 4, 4]       1,016,064\n",
            "      BatchNorm2d-61            [-1, 336, 4, 4]             672\n",
            "           Conv2d-62            [-1, 336, 4, 4]       1,016,064\n",
            "      BatchNorm2d-63            [-1, 336, 4, 4]             672\n",
            "       BasicBlock-64            [-1, 336, 4, 4]               0\n",
            "           Linear-65                   [-1, 10]           3,370\n",
            "================================================================\n",
            "Total params: 4,986,930\n",
            "Trainable params: 4,986,930\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.49\n",
            "Params size (MB): 19.02\n",
            "Estimated Total Size (MB): 26.52\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "tCaMaVVCjIUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = np.inf  # Track the highest validation accuracy\n",
        "\n",
        "start_time = time.time()\n",
        "train_loss_history, valid_loss_history = [], []\n",
        "termination_count = 0 #If valid loss does not decrease in 5 consecutive epochs terminate the training\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  print('Epoch:', epoch)\n",
        "  train_epoch(model, optimizer, scheduler, train_loader, train_loss_history, criterion)\n",
        "  evaluate(model, validation_loader, valid_loss_history, criterion)\n",
        "  if valid_loss_history[-1] < best_val_loss:\n",
        "  #Save the best model in terms of validation loss.\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    best_val_loss = valid_loss_history[-1]\n",
        "    termination_count = 0\n",
        "  else:\n",
        "    termination_count = termination_count + 1\n",
        "  if termination_count >= 10:\n",
        "    break\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "id": "P8EhRWr6pSIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YlGkSTbXnJ5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "8gNh2mxzj1A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_history,'-',linewidth=3,label='Train error')\n",
        "plt.plot(valid_loss_history,'-',linewidth=3,label='Validation error')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "%matplotlib inline\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pYXa9uWcmMTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, test_loader, [], criterion)"
      ],
      "metadata": {
        "id": "CDIQlkT7xovA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af51279b-9f21-4aa8-8ca8-6449a0c9a875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average test loss: 2.3024 Accuracy:  982/10000 (9.82%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict labels for nonlabeled dataset and save the results"
      ],
      "metadata": {
        "id": "DEdBcwR4kGRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_labels(model, data_loader):\n",
        "  model.eval()\n",
        "  predicted_labels = []\n",
        "  ids = []\n",
        "  with torch.no_grad():\n",
        "    for data, id in data_loader:\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      _, pred = torch.max(output, dim=1)\n",
        "      predicted_labels = predicted_labels + pred.tolist()\n",
        "      ids = ids + id.tolist()\n",
        "  df = pd.DataFrame({\n",
        "    'ID': ids,\n",
        "    'Labels': predicted_labels\n",
        "})\n",
        "  return df"
      ],
      "metadata": {
        "id": "MPnMLwXeDR4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "unlabeled_testset = CIFAR10Dataset(unlabeled_set_dir, train=False, unlabeled=True, transform=transform)\n",
        "unlabeled_test_loader = torch.utils.data.DataLoader(unlabeled_testset,batch_size=BATCH_SIZE_TEST,shuffle=False)"
      ],
      "metadata": {
        "id": "6-5KRy9SHqvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = predict_labels(model, unlabeled_test_loader)\n",
        "df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "FzQw8ijLJZHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}